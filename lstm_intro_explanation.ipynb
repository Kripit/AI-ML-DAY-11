{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43ffc39a",
   "metadata": {},
   "source": [
    "### ADVANCED LSTM ENERGY PREDICTION TUTORIAL - EXPLAINED LINE BY LINE\n",
    "- This is a comprehensive tutorial to learn LSTM from basic to advanced, with industry-relevant techniques\n",
    "## Modifications:\n",
    " - Uses synthetic data with realistic complexity (no external dataset)\n",
    " - Includes stacked LSTM with attention mechanism\n",
    " - Implements hyperparameter tuning (grid search)\n",
    " - Advanced preprocessing (outlier handling, feature engineering)\n",
    "- One plot (prediction_analysis.png) with attention weights, saved in main folder\n",
    "- Comprehensive line-by-line explanations for learning\n",
    "- Standalone .py format, no Jupyter dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b50ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import os \n",
    "import warnings\n",
    "import random \n",
    "from datetime import datetime , timedelta \n",
    "\n",
    "# creates time-based data for time series , generates timestamps for synthetic ddata\n",
    "\n",
    "import itertools # for generating hyperparameter combinations , enables grid search for tuning , optimizes model parameters systematically \n",
    "import logging \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0846a27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format = '%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('training.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "logger.info(\"All libraries imported successfully!\")  # Log successful imports.\n",
    "logger.info(f\"Using PyTorch version: {torch.__version__}\")  # Log PyTorch version.\n",
    "logger.info(f\"CUDA available: {torch.cuda.is_available()}\")  # Log GPU availability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56931738",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # data setting \n",
    "        \n",
    "        self.sequence_length = 24 # no of pas hrs for predictions , captures daily patterns for lstm input \n",
    "        self.prediction_steps = 1 # predict one hr ahead \n",
    "        self.train_ratio = 0.7 \n",
    "        self.val_ratio = 0.15\n",
    "        self.test_ratio = 0.15\n",
    "        \n",
    "        # model arch\n",
    "        \n",
    "        self.input_features = 10 # feeds diverse data to model , no of input features ( inc for complexity)\n",
    "        self.hidden_size = 128\n",
    "        self.num_layers = 2 # 2 lstm layers for stacked architecture , stacking layers captures deeper temporal dependencies , imporves prediction for complez time series\n",
    "        self.dropout_rate = 0.3\n",
    "        self.attention_dim = 64\n",
    "        \n",
    "        # training settings \n",
    "        \n",
    "        self.batch_size = 32\n",
    "        self.epochs=30\n",
    "        self.learning_rate = 0.001 # controls weight update speed , balances learning speed and stability\n",
    "        self.weight_decay = 1e-5\n",
    "        # l2 reg , penalizes large weights to prevent overfitting \n",
    "        \n",
    "        # hyper parameter - tuned manually before training to control how the model learns ( like learning rate , num layers , max depths etc)\n",
    "        # hyperparameter tunning = finding the best set of hyperparameters that give you the highest performance on your val/test data\n",
    "        \n",
    "        self.hyperparam_grid ={\n",
    "            'hidden_size': [128 , 256],\n",
    "            'sequence_length': [12, 24]\n",
    "        }\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        os.makedirs(\"models\", exist_ok = True)\n",
    "        \n",
    "        logger.info(f\"Configuration st! Using device : {self.device}\")\n",
    "        \n",
    "config = Config()\n",
    "\n",
    "\n",
    "# STEP 3: DATA GENERATION (Creating realistic synthetic data)\n",
    "# =============================================================================\n",
    "# WHY: Generates complex synthetic data to mimic real-world energy consumption.\n",
    "# WHAT: Creates hourly data with daily/weekly patterns, noise, and simulated outliers.\n",
    "# HOW: Uses sine functions, random noise, and outlier injection.\n",
    "# USE CASE: Simulates challenging data for industry-relevant training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2995eefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_energy_data(num_samples=10000):\n",
    "    logger.info(\"Creating synthetic energy consumption data\")\n",
    "    \n",
    "    start_date = datetime(2020, 1, 1)\n",
    "    \n",
    "    dates = [start_date + timedelta(hours=i) for i in range(num_samples)] # hourly timestamp\n",
    "    \n",
    "    hours = np.arange(num_samples)\n",
    "    \n",
    "    daily_pattern = 2.0 + 1.5 * np.sin(2 * np.pi * hours / 24) # 24hour cycle \n",
    "    \n",
    "    # 2.0 = baseline (minimum demand), 1.5 = amplitude\n",
    "    \n",
    "    weekly_pattern = 0.5 * np.sin(2 * np.pi * hours / (24 * 7))\n",
    "    \n",
    "    noise = np.random.normal(0, 0.3, num_samples) # simulates random fluctuation\n",
    "    \n",
    "    outliers = np.random.choice([1, 0], size=num_samples, p=[0.02, 0.98]) * np.random.normal(5, 1, num_samples)\n",
    "    \n",
    "    base_consumption = 3.0  # Baseline energy in kW.\n",
    "    \n",
    "    energy_consumption = base_consumption + daily_pattern + weekly_pattern + noise + outliers\n",
    "    \n",
    "    energy_consumption = np.maximum(energy_consumption, 0.5)\n",
    "\n",
    "    temperature = 20 + 10 * np.sin(2 * np.pi * hours / (24 * 365)) + np.random.normal(0, 2, num_samples)\n",
    "        \n",
    "    humidity = 50 + 20 * np.sin(2 * np.pi * hours / (24 * 365) + np.pi/2) + np.random.normal(0, 5, num_samples)\n",
    "    \n",
    "    humidity = np.clip(humidity, 10, 90)  \n",
    "    \n",
    "    voltage = 240 + np.random.normal(0, 3, num_samples)\n",
    "    \n",
    "    current = energy_consumption * 1000 / voltage\n",
    "    \n",
    "    power_factor = 0.85 + np.random.normal(0, 0.05, num_samples)\n",
    "    \n",
    "    power_factor = np.clip(power_factor, 0.7, 1.0)\n",
    "    \n",
    "    reactive_power = energy_consumption * np.tan(np.arccos(power_factor))\n",
    "    \n",
    "    day_of_week = np.array([d.weekday() for d in dates]) \n",
    "    \n",
    "    hour_of_day = np.array([d.hour for d in dates])\n",
    "    \n",
    "    is_holiday = np.random.choice([0, 1], size=num_samples, p=[0.95, 0.05])  # 5% chance of holiday.\n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        'datetime': dates,\n",
    "        'energy_consumption': energy_consumption,\n",
    "        'temperature': temperature,\n",
    "        'humidity': humidity,\n",
    "        'voltage': voltage,\n",
    "        'current': current,\n",
    "        'power_factor': power_factor,\n",
    "        'reactive_power': reactive_power,\n",
    "        'day_of_week': day_of_week,\n",
    "        'hour_of_day': hour_of_day,\n",
    "        'is_holiday': is_holiday\n",
    "    })\n",
    "    # USE CASE: Prepares data for advanced preprocessing and modeling.\n",
    "    \n",
    "    logger.info(f\"Created {len(data)} samples of synthetic energy data\")\n",
    "    logger.info(f\"Data columns: {list(data.columns)}\")\n",
    "    logger.info(f\"Data shape: {data.shape}\")\n",
    "    \n",
    "    return data \n",
    "        \n",
    "data = create_synthetic_energy_data(10000) \n",
    "\n",
    "logger.info(\"\\nFirst 5 rows of data:\")\n",
    "logger.info(f\"\\n{data.head()}\")\n",
    "\n",
    "def preprocess_data(data, config):\n",
    "    logger.info(\"Starting data preprocessing...\") \n",
    "    \n",
    "    logger.info(f\"Missing values before cleaning: {data.isnull().sum().sum()}\")\n",
    "    \n",
    "    data = data.fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # ffill = forward fill \n",
    "    #0    1.0\n",
    "    #1    1.0   # NaN replaced by previous value 1\n",
    "    #2    1.0   # NaN replaced by previous value 1\n",
    "    #3    4.0\n",
    "    #4    4.0   # NaN replaced by previous value 4\n",
    "    #5    6.0\n",
    "    # bfill = backward fill, using both first forward fill if there is still NaN left then backward fill\n",
    "    \n",
    "    logger.info(f\"Missing values after cleaning: {data.isnull().sum().sum()}\")\n",
    "    \n",
    "    # outlier detection and capping \n",
    "    q1, q3 = data['energy_consumption'].quantile([0.25, 0.75])\n",
    "    \n",
    "    # IQR (Interquartile Range) = Q3 - Q1 it tells middle spread of data\n",
    "    \n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    \n",
    "    data['energy_consumption'] = data['energy_consumption'].clip(lower=lower_bound, upper=upper_bound)\n",
    "    \n",
    "    logger.info(f\"Outliers capped for energy_consumption: Lower={lower_bound:.2f}, Upper={upper_bound:.2f}\")\n",
    "\n",
    "    # feature engineering\n",
    "    data['hour_sin'] = np.sin(2 * np.pi * data['hour_of_day'] / 24)\n",
    "    \n",
    "    data['hour_cos'] = np.cos(2 * np.pi * data['hour_of_day'] / 24)\n",
    "    \n",
    "    data['day_sin'] = np.sin(2 * np.pi * data['day_of_week'] / 7)\n",
    "    \n",
    "    data['day_cos'] = np.cos(2 * np.pi * data['day_of_week'] / 7)\n",
    "    \n",
    "    data['energy_lag_1'] = data['energy_consumption'].shift(1)\n",
    "    \n",
    "    data['energy_lag_24'] = data['energy_consumption'].shift(24)\n",
    "    \n",
    "    data['energy_rolling_mean'] = data['energy_consumption'].rolling(window=24).mean()\n",
    "    # 24-hour rolling mean.\n",
    "    \n",
    "    data = data.dropna() # Remove NaN rows from lag and rolling features\n",
    "    \n",
    "    logger.info(f\"Data shape after feature engineering: {data.shape}\")\n",
    "    \n",
    "    feature_columns = [\n",
    "        'temperature', 'humidity', 'voltage', 'current', 'power_factor', 'reactive_power',\n",
    "        'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'energy_lag_1', 'energy_lag_24', 'energy_rolling_mean', 'is_holiday'\n",
    "    ]\n",
    "    \n",
    "    logger.info(f\"Selected {len(feature_columns)} features: {feature_columns}\")\n",
    "    \n",
    "    n_samples = len(data)\n",
    "    train_size = int(n_samples * config.train_ratio)\n",
    "    val_size = int(n_samples * config.val_ratio)\n",
    "    train_data = data[:train_size].copy()\n",
    "    val_data = data[train_size:train_size + val_size].copy()\n",
    "    test_data = data[train_size + val_size:].copy()\n",
    "    logger.info(f\"Data split - Train: {len(train_data)}, Validation: {len(val_data)}, Test: {len(test_data)}\")\n",
    "    \n",
    "    # Data normalization\n",
    "    feature_scaler = StandardScaler() # Standardize features (mean=0, std=1)\n",
    "    target_scaler = MinMaxScaler() # Scale target to [0,1]\n",
    "    X_train = feature_scaler.fit_transform(train_data[feature_columns])\n",
    "    y_train = target_scaler.fit_transform(train_data[['energy_consumption']]).flatten()\n",
    "    \n",
    "    X_val = feature_scaler.transform(val_data[feature_columns])\n",
    "    y_val = target_scaler.transform(val_data[['energy_consumption']]).flatten()\n",
    "    X_test = feature_scaler.transform(test_data[feature_columns])\n",
    "    y_test = target_scaler.transform(test_data[['energy_consumption']]).flatten()\n",
    "    \n",
    "    logger.info(\"Data scaling completed\")\n",
    "    logger.info(f\"Feature shape: {X_train.shape}, Target shape: {y_train.shape}\")\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train, 'y_train': y_train,\n",
    "        'X_val': X_val, 'y_val': y_val,\n",
    "        'X_test': X_test, 'y_test': y_test,\n",
    "        'feature_scaler': feature_scaler,\n",
    "        'target_scaler': target_scaler,\n",
    "        'feature_columns': feature_columns\n",
    "    }\n",
    "    \n",
    "processed_data = preprocess_data(data, config)   \n",
    "\n",
    "# dataset class formatting data for pytorch \n",
    "class EnergyDataset(Dataset):\n",
    "    def __init__(self, X, y, sequence_length):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.sequence_length = sequence_length\n",
    "        self.sequences = []\n",
    "        self.targets = []\n",
    "        \n",
    "        for i in range(len(X) - sequence_length):\n",
    "            sequence = X[i:i + sequence_length]\n",
    "            target = y[i + sequence_length]\n",
    "            self.sequences.append(sequence)\n",
    "            self.targets.append(target)\n",
    "            \n",
    "        self.sequences = np.array(self.sequences, dtype=np.float32)\n",
    "        self.targets = np.array(self.targets, dtype=np.float32)\n",
    "        logger.info(f\"Created {len(self.sequences)} sequences with length {sequence_length}\")\n",
    "        logger.info(f\"Sequence shape: {self.sequences.shape}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = torch.tensor(self.sequences[idx], dtype=torch.float32)\n",
    "        target = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "        return sequence, target\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_dim = attention_dim\n",
    "        \n",
    "        self.attention = nn.Linear(hidden_size, attention_dim)\n",
    "        self.value = nn.Linear(attention_dim, 1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, lstm_output):\n",
    "        # lstm_output: (batch_size, seq_len, hidden_size)\n",
    "        attention_scores = self.attention(lstm_output)  # (batch_size, seq_len, attention_dim)\n",
    "        attention_weights = self.value(attention_scores)  # (batch_size, seq_len, 1)\n",
    "        attention_weights = self.softmax(attention_weights)  # (batch_size, seq_len, 1)\n",
    "        context_vector = torch.bmm(attention_weights.transpose(1, 2), lstm_output).squeeze(1)  # (batch_size, hidden_size)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class EnergyLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_rate, attention_dim):\n",
    "        super(EnergyLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # stacked LSTM \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                            batch_first=True, dropout=dropout_rate)\n",
    "        \n",
    "        self.attention = Attention(hidden_size, attention_dim)\n",
    "        \n",
    "        # dense layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.fc2 = nn.Linear(hidden_size // 2, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        # WHY: Dense layers refine attention output for final prediction.\n",
    "        \n",
    "        logger.info(f\"Advanced LSTM created: Input={input_size}, Hidden={hidden_size}, Layers={num_layers}, Attention_dim={attention_dim}\")\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        logger.info(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        lstm_out, (h_n, c_n) = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        context_vector, attention_weights = self.attention(lstm_out) \n",
    "        \n",
    "        x = self.dropout(context_vector)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        output = self.fc2(x)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def init_weights(self):\n",
    "        # WHY: Initializes weights for stable training.\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                if 'lstm' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                else:\n",
    "                    nn.init.xavier_normal_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for sequences, targets in train_loader:\n",
    "        sequences, targets = sequences.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions, _ = model(sequences)  # Ignore attention weights during training.\n",
    "        predictions = predictions.squeeze()\n",
    "        loss = criterion(predictions, targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Prevent exploding gradients.\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    logger.info(f\"Train Loss: {avg_loss:.6f}\")\n",
    "    return avg_loss\n",
    "\n",
    "def validate_one_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, targets in val_loader:\n",
    "            sequences, targets = sequences.to(device), targets.to(device)\n",
    "            predictions, _ = model(sequences)\n",
    "            predictions = predictions.squeeze()\n",
    "            loss = criterion(predictions, targets)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    logger.info(f\"Validation Loss: {avg_loss:.6f}\")\n",
    "    return avg_loss               \n",
    "\n",
    "def evaluate_model(model, test_loader, target_scaler, device):\n",
    "    model.eval()\n",
    "    predictions, actuals, attention_weights = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, targets in test_loader:\n",
    "            sequences, targets = sequences.to(device), targets.to(device)\n",
    "            outputs, attn_weights = model(sequences)\n",
    "            predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "            actuals.extend(targets.cpu().numpy())\n",
    "            attention_weights.extend(attn_weights.cpu().numpy())\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    actuals = np.array(actuals)\n",
    "    attention_weights = np.array(attention_weights)\n",
    "    \n",
    "    # Convert to original scale\n",
    "    predictions_original = target_scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "    actuals_original = target_scaler.inverse_transform(actuals.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    mse = mean_squared_error(actuals_original, predictions_original)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actuals_original, predictions_original)\n",
    "    mape = np.mean(np.abs((actuals_original - predictions_original) / actuals_original)) * 100\n",
    "    r2 = r2_score(actuals_original, predictions_original)\n",
    "    \n",
    "    logger.info(\"\\nMODEL PERFORMANCE METRICS\")\n",
    "    logger.info(f\"Mean Squared Error (MSE):        {mse:.6f} kWÂ²\")\n",
    "    logger.info(f\"Root Mean Squared Error (RMSE):  {rmse:.6f} kW\")\n",
    "    logger.info(f\"Mean Absolute Error (MAE):       {mae:.6f} kW\")\n",
    "    logger.info(f\"Mean Absolute Percentage Error:  {mape:.2f}%\")\n",
    "    logger.info(f\"RÂ² Score:                        {r2:.6f}\")\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions_original,\n",
    "        'actuals': actuals_original,\n",
    "        'attention_weights': attention_weights,\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'mape': mape,\n",
    "        'r2': r2\n",
    "    }\n",
    "\n",
    "# hyperparameter tuning grid search for optimization, find optimal model parameters for best performance \n",
    "def hyperparameter_tuning(processed_data, config):\n",
    "    best_val_loss = float('inf')\n",
    "    best_params = None\n",
    "    best_model_state = None \n",
    "    best_results = None\n",
    "    \n",
    "    param_combinations = list(itertools.product(\n",
    "        config.hyperparam_grid['hidden_size'],\n",
    "        config.hyperparam_grid['sequence_length']\n",
    "    ))\n",
    "    \n",
    "    for hidden_size, sequence_length in param_combinations:\n",
    "        logger.info(f\"\\nTesting hidden_size={hidden_size}, sequence_length={sequence_length}\")\n",
    "        \n",
    "        train_dataset = EnergyDataset(processed_data['X_train'], processed_data['y_train'], sequence_length)\n",
    "        val_dataset = EnergyDataset(processed_data['X_val'], processed_data['y_val'], sequence_length)\n",
    "        test_dataset = EnergyDataset(processed_data['X_test'], processed_data['y_test'], sequence_length) \n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "        \n",
    "        # Initializing model\n",
    "        model = EnergyLSTM(\n",
    "            input_size=len(processed_data['feature_columns']),\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=config.num_layers,\n",
    "            dropout_rate=config.dropout_rate,\n",
    "            attention_dim=config.attention_dim\n",
    "        )\n",
    "        \n",
    "        model.init_weights()\n",
    "        model = model.to(config.device)\n",
    "        \n",
    "        # defining loss and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "        \n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                        mode='min', factor=0.5, patience=3, min_lr=1e-6)\n",
    "        \n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        patience_counter = 0\n",
    "        early_stopping_patience = 5\n",
    "        \n",
    "        for epoch in range(config.epochs):\n",
    "            logger.info(f\"Epoch [{epoch + 1}/{config.epochs}]\")\n",
    "            train_loss = train_one_epoch(model, train_loader, criterion, optimizer, config.device)\n",
    "            val_loss = validate_one_epoch(model, val_loader, criterion, config.device)\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_params = {'hidden_size': hidden_size, 'sequence_length': sequence_length}\n",
    "                best_model_state = model.state_dict()\n",
    "                test_results = evaluate_model(model, test_loader, processed_data['target_scaler'], config.device)\n",
    "                best_results = test_results\n",
    "                patience_counter = 0\n",
    "                logger.info(f\"âœ“ New best model! Validation loss: {val_loss:.6f}\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                logger.info(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "                break\n",
    "            \n",
    "            if config.device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    torch.save({\n",
    "        'model_state_dict': best_model_state,\n",
    "        'params': best_params,\n",
    "        'val_loss': best_val_loss\n",
    "    }, 'models/best_energy_lstm_model.pth')\n",
    "    logger.info(f\"Best model saved with params: {best_params}, Val Loss: {best_val_loss:.6f}\")\n",
    "    \n",
    "    return best_params, best_results, train_losses, val_losses\n",
    "\n",
    "# Run hyperparameter tuning\n",
    "best_params, test_results, train_losses, val_losses = hyperparameter_tuning(processed_data, config)\n",
    "# WHY: Optimizes model for best performance.\n",
    "# USE CASE: Ensures industry-grade accuracy.\n",
    "\n",
    "model = EnergyLSTM(\n",
    "    input_size=len(processed_data['feature_columns']),\n",
    "    hidden_size=best_params['hidden_size'],\n",
    "    num_layers=config.num_layers,\n",
    "    dropout_rate=config.dropout_rate,\n",
    "    attention_dim=config.attention_dim\n",
    ")\n",
    "model.load_state_dict(torch.load('models/best_energy_lstm_model.pth', map_location=config.device)['model_state_dict'])\n",
    "model = model.to(config.device)\n",
    "logger.info(\"Best model reloaded for visualization\")\n",
    "\n",
    "test_dataset = EnergyDataset(processed_data['X_test'], processed_data['y_test'], best_params['sequence_length'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# Re-evaluate for consistency\n",
    "test_results = evaluate_model(model, test_loader, processed_data['target_scaler'], config.device)\n",
    "\n",
    "def plot_predictions(results, sequence_length, num_samples=200):\n",
    "    logger.info(\"Creating advanced prediction plot...\")\n",
    "    \n",
    "    predictions = results['predictions'][:num_samples]\n",
    "    actuals = results['actuals'][:num_samples]\n",
    "    attention_weights = results['attention_weights'][:num_samples]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))  # Single plot for predictions and attention.\n",
    "    plt.title('Advanced LSTM: Energy Consumption Predictions', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Plot predictions vs actuals\n",
    "    time_index = range(len(predictions))\n",
    "    plt.plot(time_index, actuals, label='Actual', color='blue', linewidth=1.5)\n",
    "    plt.plot(time_index, predictions, label='Predicted', color='red', linewidth=1.5)\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Energy Consumption (kW)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add metrics\n",
    "    plt.text(0.02, 0.98, f'MAE: {results[\"mae\"]:.3f} kW\\nRÂ²: {results[\"r2\"]:.3f}',\n",
    "             transform=plt.gca().transAxes, fontsize=10, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('prediction_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot attention weights for first sample\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.title('Attention Weights for First Test Sample', fontsize=12, fontweight='bold')\n",
    "    plt.plot(range(sequence_length), attention_weights[0].mean(axis=-1), color='green')\n",
    "    plt.xlabel('Time Step in Sequence')\n",
    "    plt.ylabel('Attention Weight')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('attention_weights.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    logger.info(\"Plots saved: prediction_analysis.png, attention_weights.png\")\n",
    "\n",
    "# Create visualization\n",
    "plot_predictions(test_results, best_params['sequence_length'])\n",
    "\n",
    "def predict_next_hour(model, recent_data, feature_scaler, target_scaler, device, sequence_length):\n",
    "    logger.info(\"Making real-time prediction for next hour...\")\n",
    "    \n",
    "    if len(recent_data) < sequence_length:\n",
    "        raise ValueError(f\"Need at least {sequence_length} hours, got {len(recent_data)}\")\n",
    "    \n",
    "    input_sequence = pd.DataFrame(recent_data, columns=processed_data['feature_columns'])\n",
    "    input_scaled = feature_scaler.transform(input_sequence)\n",
    "    input_tensor = torch.tensor(input_scaled, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction, _ = model(input_tensor)\n",
    "        prediction = prediction.squeeze().cpu().numpy()\n",
    "        \n",
    "    prediction_original = target_scaler.inverse_transform([[prediction]])[0][0]\n",
    "    logger.info(f\"Predicted energy consumption: {prediction_original:.3f} kW\")\n",
    "    return prediction_original  \n",
    "\n",
    "logger.info(\"\\nDemonstrating real-time prediction...\")\n",
    "test_features = pd.DataFrame(processed_data['X_test'][-best_params['sequence_length']:], \n",
    "                            columns=processed_data['feature_columns'])\n",
    "test_actual = processed_data['y_test'][-1]\n",
    "actual_original = processed_data['target_scaler'].inverse_transform([[test_actual]])[0][0]\n",
    "predicted = predict_next_hour(model, test_features, processed_data['feature_scaler'],\n",
    "                             processed_data['target_scaler'], config.device, best_params['sequence_length'])\n",
    "logger.info(f\"Actual energy consumption: {actual_original:.3f} kW\")\n",
    "logger.info(f\"Prediction error: {abs(predicted - actual_original):.3f} kW \"\n",
    "            f\"({abs(predicted - actual_original)/actual_original*100:.1f}%)\")\n",
    "\n",
    "def generate_comprehensive_report(config, best_params, test_results):\n",
    "    report = f\"\"\"\n",
    "{'='*60}\n",
    "                ADVANCED LSTM ENERGY PREDICTION REPORT\n",
    "{'='*60}\n",
    "\n",
    "1. PROJECT OVERVIEW\n",
    "{'-'*40}\n",
    "â€¢ Objective: Predict hourly energy consumption using advanced LSTM with attention\n",
    "â€¢ Model Type: Stacked LSTM with Attention Mechanism\n",
    "â€¢ Dataset: Synthetic energy data with outliers and complex features\n",
    "â€¢ Target Variable: Energy consumption (kW)\n",
    "\n",
    "2. DATA CHARACTERISTICS\n",
    "{'-'*40}\n",
    "â€¢ Total Samples: {len(data):,}\n",
    "â€¢ Features: {len(processed_data['feature_columns'])}\n",
    "â€¢ Best Sequence Length: {best_params['sequence_length']} hours\n",
    "â€¢ Train/Val/Test Split: {config.train_ratio:.1%}/{config.val_ratio:.1%}/{config.test_ratio:.1%}\n",
    "\n",
    "3. MODEL ARCHITECTURE\n",
    "{'-'*40}\n",
    "â€¢ Input Size: {len(processed_data['feature_columns'])} features\n",
    "â€¢ Best Hidden Size: {best_params['hidden_size']} neurons\n",
    "â€¢ LSTM Layers: {config.num_layers}\n",
    "â€¢ Attention Dimension: {config.attention_dim}\n",
    "â€¢ Dropout Rate: {config.dropout_rate}\n",
    "\n",
    "4. TRAINING CONFIGURATION\n",
    "{'-'*40}\n",
    "â€¢ Optimizer: AdamW\n",
    "â€¢ Learning Rate: {config.learning_rate}\n",
    "â€¢ Batch Size: {config.batch_size}\n",
    "â€¢ Epochs: {config.epochs}\n",
    "\n",
    "5. TEST SET PERFORMANCE\n",
    "{'-'*40}\n",
    "â€¢ RMSE: {test_results['rmse']:.4f} kW\n",
    "â€¢ MAE: {test_results['mae']:.4f} kW\n",
    "â€¢ MAPE: {test_results['mape']:.2f}%\n",
    "â€¢ RÂ² Score: {test_results['r2']:.4f}\n",
    "\n",
    "6. FILES GENERATED\n",
    "{'-'*40}\n",
    "â€¢ Model: models/best_energy_lstm_model.pth\n",
    "â€¢ Plots: prediction_analysis.png, attention_weights.png\n",
    "â€¢ Log: training.log\n",
    "\"\"\"\n",
    "    \n",
    "    with open('comprehensive_report.txt', 'w') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    logger.info(report)\n",
    "    logger.info(\"Comprehensive report saved to 'comprehensive_report.txt'\")\n",
    "\n",
    "# Generate report\n",
    "generate_comprehensive_report(config, best_params, test_results)\n",
    "\n",
    "logger.info(\"\\nðŸŽ‰ CONGRATULATIONS! ðŸŽ‰\")\n",
    "logger.info(\"You have completed an advanced LSTM energy prediction project!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288eeb9d",
   "metadata": {},
   "source": [
    "**Mathematical Breakdown:**\n",
    "- **`2.0`** = **Baseline consumption** (minimum energy demand)\n",
    "- **`1.5`** = **Amplitude** (peak variation from baseline)\n",
    "- **`2Ï€`** = **Full cycle** (360 degrees in radians)\n",
    "- **`hours / 24`** = **Normalization** (maps 24 hours to one complete sine cycle)\n",
    "\n",
    "**Why This Works:**\n",
    "- **Hour 0 (midnight)**: `sin(0) = 0` â†’ Low consumption\n",
    "- **Hour 6 (dawn)**: `sin(Ï€/2) = 1` â†’ Peak morning consumption  \n",
    "- **Hour 12 (noon)**: `sin(Ï€) = 0` â†’ Moderate consumption\n",
    "- **Hour 18 (evening)**: `sin(3Ï€/2) = -1` â†’ Minimum consumption\n",
    "- **Hour 24**: Returns to `sin(0) = 0` â†’ **Cyclical continuity**\n",
    "\n",
    "**Weekly Pattern:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380dce68",
   "metadata": {},
   "source": [
    "- **Period**: 168 hours (7 days Ã— 24 hours)\n",
    "- **Lower amplitude** (0.5) represents subtle weekly variations\n",
    "- **Captures weekend vs weekday differences**\n",
    "\n",
    "###  Cyclical Feature Encoding\n",
    "\n",
    "**The Problem with Linear Time:**\n",
    "- Hour 23 â†’ 0: Appears as huge jump (23 units)\n",
    "- Actual relationship: Very close in time (1 hour difference)\n",
    "\n",
    "**Sine-Cosine Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5f755b",
   "metadata": {},
   "source": [
    "**Mathematical Benefits:**\n",
    "- **Continuous representation**: No artificial boundaries\n",
    "- **Distance preservation**: Similar hours have similar values\n",
    "- **2D embedding**: `(sin, cos)` creates circular representation\n",
    "- **ML-friendly**: Neural networks handle continuous features better\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403c3f66",
   "metadata": {},
   "source": [
    "**Statistical Rationale:**\n",
    "- **Î¼ = 0**: Zero-mean ensures no systematic bias\n",
    "- **Ïƒ = 0.3**: Standard deviation represents real-world measurement uncertainty\n",
    "- **Normal distribution**: Models aggregate effect of many small random factors\n",
    "- **Central Limit Theorem**: Real energy fluctuations approximate Gaussian distribution\n",
    "\n",
    "### Outlier Simulation\n",
    "\n",
    "**Statistical Theory:**\n",
    "- **Robust to distribution**: Works for non-normal distributions\n",
    "- **1.5 Ã— IQR rule**: Standard statistical practice\n",
    "- **Outlier definition**: Values beyond whiskers of box plot\n",
    "- **Data integrity**: Preserves 99.3% of normal distribution data\n",
    "\n",
    "###  Missing Value Strategies\n",
    "\n",
    "**Forward Fill â†’ Backward Fill:**\n",
    "**For Energy Consumption:**\n",
    "- **[0,1] range**: Bounded output suitable for sigmoid activation\n",
    "- **Preserves relationships**: Maintains relative differences\n",
    "- **Numerical stability**: Prevents extreme target values\n",
    "- **Easy interpretation**: Direct scaling back to original units\n",
    "\n",
    "---\n",
    "\n",
    "##  LSTM Architecture Theory\n",
    "\n",
    "### Long Short-Term Memory Cells\n",
    "\n",
    "**Why LSTM for Energy Forecasting:**\n",
    "- **Long-term dependencies**: Remembers patterns from hours/days ago\n",
    "- **Gradient flow**: Solves vanishing gradient problem\n",
    "- **Selective memory**: Gates decide what information to keep/forget\n",
    "- **Sequential processing**: Natural fit for time series data\n",
    "\n",
    "**2-Layer LSTM Design:**\n",
    "- **Layer 1**: Learns basic temporal patterns\n",
    "- **Layer 2**: Learns complex pattern combinations\n",
    "- **Hierarchical learning**: Lower layers capture simple patterns, upper layers capture complex interactions\n",
    "- **Representation depth**: Multiple abstraction levels\n",
    "\n",
    "\n",
    "**Attention Benefits:**\n",
    "- **Selective focus**: Emphasizes relevant time steps\n",
    "- **Long sequences**: Handles variable-length dependencies\n",
    "- **Interpretability**: Attention weights show model focus\n",
    "- **Performance**: Often outperforms standard LSTM\n",
    "\n",
    "**Energy Forecasting Context:**\n",
    "- **Peak hours**: Model learns to focus on similar historical hours\n",
    "- **Weekly patterns**: Attention to same weekday historical data\n",
    "- **Event correlation**: Links current conditions to similar past events\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d12c061",
   "metadata": {},
   "source": [
    "### Internal working of nn.lstm\n",
    "\n",
    "- f_t = sigmoid(W_f * x_t + U_f * h_{t-1} + b_f)\n",
    "- i_t = sigmoid(W_i * x_t + U_i * h_{t-1} + b_i)\n",
    "- g_t = tanh(W_g * x_t + U_g * h_{t-1} + b_g)\n",
    "- c_t = f_t * c_{t-1} + i_t * g_t\n",
    "- o_t = sigmoid(W_o * x_t + U_o * h_{t-1} + b_o)\n",
    "- h_t = o_t * tanh(c_t)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
